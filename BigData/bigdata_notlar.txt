--Big Data--
Volume: Verinin hacmi
Velocity: Verinin artış hızı
Variety: Veri çeşitliliği(resim,müzik, video)
Verification: Doğrulama - Verinin temizlenmesi - Veri madencileri buna kirli,gürültülü veri der.
                            ör: hız sensöüründen gelen - değeri işlememeliyiz.
Value: verinin değerli olması. Şirkete artı katan veri.

Apache Spark hadoopdan çok daha hızlıdır.

Kafka : Büyük verileri belli topiklerde toplamak ve analiz etmek ve bu verileri diğer sistemlere transfer edebilmek için bir mesajlaşma(queue) sistemidir.

nosql = cassandra, Clustrix, Couchbase, Amazon DynmicDb, Apache Hbase, ibmCludant, MarkLogic, OracleNoSql,memsql, mongodb,nuodb,
        Riak, Splice Machine, TransLattice, VoltDb, Aerospike

Hadoop'u cluster yaparsak sistem sonuca daha hızlı ulaşıyor. 5 cluster sistem bir işlemi 5 kat daha hızlı yapar.
örneğin 1 ile 100 arasındaki asal sayıları bulmak için sistem bu 5 makinaya işi böler ve result daha hızlı olur.
hadop 4 modülden oluşur
    - Common (bazı modüllerin hadoopa erişmesi için gerekli kütüphane.)
    - HDFS (verileri saklama sistemi)
    - YARN(ne kadar cpu,ram kullanması gerektiğini hesaplar.)
    - MapReduce (verileri işlememize yarayan araçlar)
Hadoop, Spark, Pig, Hive kurmak için Cloudera ve Hortonwork kullanılabilir.
https://www.cloudera.com/downloads/quickstart_vms/5-13.html
Quick start linkinden virtualbox,vmware,kvm,docker image kurulabilir.

https://yadi.sk/d/Ue-71ww23TaPRf
HDFS Komutları:
https://github.com/talhaklc/HDFS-Komutlari

Apache Pig : hadoop için sorgulamalar vs.
https://github.com/talhaklc/Apache-Pig-Kodlari
https://github.com/talhaklc/Apache-Pig-Operatorler
https://github.com/talhaklc/Apache-Pig-Fonksiyonlar
https://github.com/talhaklc/Apache-Pig-Join





ödev
https://yadi.sk/d/KYQurpA23V34Wg
Soru 1
Titanik’deki yolcu sayısını bulunuz.
Soru 2
Titanik’deki yolcuların kaç tanesi kurtulmuştur ?
Soru 3
Kurtulan yolcuların kaç tanesi erkek, kaç tanesi kadındır ?
Soru 4
Hayatını kaybeden yolcu sayısı kaçtır ?
Soru 5
Hayatını kaybeden çocuk yolcuların isimlerini bulunuz..

0-7 Yaş aralığını çocuk kabul ediniz.

--Apache Hive--
Büyük veriler ile hadoop üzerinde işlem yapmak için kullanılır.
SQl sorguları yazılı.
1- VErileri hdfs'e at 2- veritabanı oluştur  3- Tablo Oluştur 4- Sql sorgusu oluştur.

hdfs dfs -copyFromLocal /home/cloudera/Desktop/movies.csv /hiveexam

veritabanı oluşturmak için browserda hue linkine tıkla. + ile veritabanı oluştur. Dbye gir table oluştur ve hdfs deki csvyi sec.
query ile sql sorguları artık yazılabilir.

Select * from movie.movies
Select title from movie.movies
Select title,genres from movie.movies where genres='Comedy'
Select title,genres from movie.movies where genres LIKE '%Comedy%'
Select count(title) from movie.movies where genres LIKE '%Comedy%'
Select title,genres from movie.movies where title LIKE '%1996%'
Select title,genres from movie.movies where title LIKE '%1996%' AND genres LIKE '%Comedy%'
Select count(title) from movie.movies where title LIKE '%1996%' AND genres LIKE '%Comedy%'

ödev
https://yadi.sk/d/NBeURuPa3V3Rp3
Soru 1
Toplam kaç yorum yapılmıştır ?
Soru 2
Hangi şehirden kaç yorum yapılmıştır ?
Soru 3
Kullanıcı yorumlarından olumlu olan yorumları seçiniz

Spark,Hadooptan daha hızlıdır.Çünkü veri saklama birimi olmadığı için Memory'de (Ram üzerinde) çalışır.

--MongoDb--
document-oriented,her kayıt bir dökümandır,dökümanlar json olarak saklanır.ilişti kurmaya gerek yok.
cluster yapılabilir, replication yapabiliyoruz. javada primefaces ile, nodejs ile entegre edilebilir.

MongoDB Windows Kurulumu
MongoDB'yi bilgisayarımıza kurmak için, ilk olarak MongoDB Download sayfasına gidelim.
Buradan işletim sistemimize uygun olan dosyayı bilgisayarımıza indirelim.
İndirdiğimiz dosyayı çalıştıralım ve Next>Lisansı kabul edelim>Next diyerek ilerleyelim.
Bu aşamaya Complete seçeneğini seçelim.Sol alt tarafta bulunan Install MongoDB Compass seçeneğindeki tiki kaldıralım ve Next diyelim.
C:'nin altında mongodb adında klasör oluşturalım ve içerisine mongo.config ve mongo.log adında iki dosya oluşturalım.
C:'nin altında ki mongodb klasörünün içerisine data adında klasör oluşturalım
mongo.config dosyasını açalım ve içerisine ;
dbpath = C:\mongodb\data
logpath = C:\mongodb\mongo.log         satırlarını ekleyelim.
 Kurulum işlemi tamamlandı.

MongoDB sunucusunu çalıştırmak için ;
Windows komut işlemini açalım ( cmd ) 
Komut satırında mongo kurulum path'ine geldikten sonra ( cd C:\mongodb )
mongod.exe --config C:\mongodb\mongo.config        satırını yazarak enter'a basalım.

arayüz olarak Robo 3T kullanılmalı...

https://github.com/talhaklc/Mongodb-Java-Enteg
java Maven-- kütüphane bağımlılıklarını ortadan kaldırır. pom.xml e isteiğin kütüphaneyi yaz o netten indirir.
googledan aşağıdaki dependecy bulduk.
https://mvnrepository.com/artifact/org.mongodb/mongo-java-driver
pom.xml e dependecy tagını ekledik. Spring Tool Suit menulerden Project + Build yapılınca yeni eklenen dependecyler indirilir.
https://github.com/talhaklc/Mongodb-Java-Enteg

-- Elasticsearch --
https://www.elastic.co/
https://www.elastic.co/guide/en/x-pack/current/security-getting-started.html
https://www.elastic.co/downloads/elasticsearch

Cluster yapısı var. monitöring için Kibana kullan. elastic sitesinde var.

Elasticsearch herhangi bir konfigürasyon yapmadan kurulduğunda default olarak 5 shard dan oluşur.
Elasticsearch'de index oluşturmak için GET /IndexName
Elastic de nosql bir db dir. Çok hızlı okuma yapar. Çünkü veriler yazılırken indexleme yapılır. Ama yazma hızı yavaştır.

Windows'a kurarken zipi indir. dizine çıkart. komut isteminden cd ile içine gir ve bin\elasticsearch.bat dosyasını çalıştır.
started görürsen artık elasticsearch'e kayıt ekleyip silip arattırabiliriz.

postman kullanarak elasticsearch'e istek göndereceğiz.


DataBase -> Index
Table    -> Type
Row      -> Document
Column   -> Fields
Schema   -> Mapping

.net core uygulama kurulum vs.
ELK seriloglama

https://medium.com/devopsturkiye/net-core-ile-docker-%C3%BCzerinde-%C3%A7al%C4%B1%C5%9Fan-elasticsearche-loglama-nas%C4%B1l-yap%C4%B1l%C4%B1r-fcb60386e339
https://medium.com/devopsturkiye/net-core-3-0-elasticsearch-kurulum-konfig%C3%BCrasyon-ve-sorgulama-3dc81e4fcc72

verileri json şeklinde tutar.
Kurulum için
https://umutcakir.wordpress.com/2017/02/25/ubuntu-16-04-server-uzerinde-elasticsearch-kurulumu-ve-ram-kullanimi-ayarlariheap-size/
https://www.elastic.co/downloads/elasticsearch
https://hub.docker.com/_/elasticsearch
Sunucu 9200 portundan çalışıyor. POST,GET,PUT,DELETE işlemleri yapılabilir. API yazmış gibi olduk.:)
Postmen ile test yapılabilir. örnek GET localhost:9200/_all  //tüm veritabanı bilgilerini verir.
GET localhost:9200/exam/product/_search?q=brand:apple  //tüm apple markalı ürünleri listeler.
https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html
docker container run -d elasticsearch -p 9200:9200 --name elastic 
java Maven ile yazarken elasticsearch verionu kaçsa java versionunu ona göre seçmek lazım
dependecy olarak Mavenrepository.com dan Transport seçilmeli.
https://github.com/talhaklc/elasticsearch-java-ent
http://www.borakasmer.com/net-core-uzerinde-elasticsearch-ile-istenen-bir-mesafe-icindeki-lokasyonlarin-filitrelenmesi/


-- Apache Kafka --
Scala dili ile yazılmış...default port 9092 zookeeper ile kullanılıyor.(port 2181)
Büyük veriyi hatasız ve hızlı toplamak için kullanılan bir kuyruk mekanizmasıdır.
https://kafka.apache.org/downloads
İndirdiğimiz dosyayı Zip'ten çıkartıp C: dizinine kopyalayacağız.
Config klasörünün içerisindeki server.properties dosyasını güncelleyeceğiz ;
log.dirs=C:\kafka_2.10-0.10.1.0\kafka-logs bölümünü ekleyeceğiz.

https://zookeeper.apache.org/releases.html#download
indirdiğimiz dosyayı Zip'ten çıkartıp C: dizinine kopyalayacağız.
Config klasörünün içerisindeki zoo_sample.cfg dosya ismini zoo.cfg olarak güncelleyeceğiz
zoo.cfg dosyasını güncelleyeceğiz ; 
dataDir=C:\zookeeper-3.3.6\data
Bilgisayarım>Sağ Klik>Özellikler>Gelişmiş Sistem Ayarları>Ortam Değişkenleri yolunu izleyeceğiz
Path bölümüne C:\zookeeper-3.4.10\bin ekleyeceğiz
Apache Kafka Server'ını çalıştırmak için öncelikle Zookeeper'ı çalıştırmalısınız.
https://github.com/talhaklc/ProducerExample


-- Apache Spark --
Büyük veri analizlerinde kullanılır. Spark in-memory çalışır. veri analizi ram üzreinde gerçekleşir.
Spark hadoop a göre 100 kat daha hızlıdır. Spark streaming sayesinde milyonlarca veriyi anlık analiz edebilir.
MLlib kütüphanesi ile Machine Learning yapılabilir.
https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/
Java Maven için kullanacağımız kütüphane


https://github.com/talhaklc/SparkWorldCup
Veriyi RDD ye atmak için java maven kullanacığız.
https://mvnrepository.com/artifact/org.apache.spark/spark-core

public class App {
    public static void main (String[] args){
        JavaSparkContext cont = new JavaSparkContext("local",First Exam");
        javaRDD<string> wordadd = cont.textFile("c:\\Users\\oncell\\Desktop\\birdosya.txt);
        System.out.println(wordadd.count()); //wordadd. dediğimze bir sürü analiz için metodlar çıkıyor.
        
    }
}

RDD veri yükleme, RDD Map Metodu, Rdd Filter metodu.FlatMap,MapPartition,Distinct,Sample Methodu
Pair RDD
https://github.com/talhaklc/SparkWorldCup
