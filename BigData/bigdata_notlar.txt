--Big Data--
Volume: Verinin hacmi
Velocity: Verinin artış hızı
Variety: Veri çeşitliliği(resim,müzik, video)
Verification: Doğrulama - Verinin temizlenmesi - Veri madencileri buna kirli,gürültülü veri der.
                            ör: hız sensöüründen gelen - değeri işlememeliyiz.
Value: verinin değerli olması. Şirkete artı katan veri.

Apache Spark hadoopdan çok daha hızlıdır.

Kafka : Büyük verileri belli topiklerde toplamak ve analiz etmek ve bu verileri diğer sistemlere transfer edebilmek için bir mesajlaşma(queue) sistemidir.

nosql = cassandra, Clustrix, Couchbase, Amazon DynmicDb, Apache Hbase, ibmCludant, MarkLogic, OracleNoSql,memsql, mongodb,nuodb,
        Riak, Splice Machine, TransLattice, VoltDb, Aerospike

Hadoop'u cluster yaparsak sistem sonuca daha hızlı ulaşıyor. 5 cluster sistem bir işlemi 5 kat daha hızlı yapar.
örneğin 1 ile 100 arasındaki asal sayıları bulmak için sistem bu 5 makinaya işi böler ve result daha hızlı olur.
hadop 4 modülden oluşur
    - Common (bazı modüllerin hadoopa erişmesi için gerekli kütüphane.)
    - HDFS (verileri saklama sistemi)
    - YARN(ne kadar cpu,ram kullanması gerektiğini hesaplar.)
    - MapReduce (verileri işlememize yarayan araçlar)
Hadoop, Spark, Pig, Hive kurmak için Cloudera ve Hortonwork kullanılabilir.
https://www.cloudera.com/downloads/quickstart_vms/5-13.html
Quick start linkinden virtualbox,vmware,kvm,docker image kurulabilir.

https://yadi.sk/d/Ue-71ww23TaPRf
HDFS Komutları:
https://github.com/talhaklc/HDFS-Komutlari

Apache Pig : hadoop için sorgulamalar vs.
https://github.com/talhaklc/Apache-Pig-Kodlari
https://github.com/talhaklc/Apache-Pig-Operatorler
https://github.com/talhaklc/Apache-Pig-Fonksiyonlar
https://github.com/talhaklc/Apache-Pig-Join





ödev
https://yadi.sk/d/KYQurpA23V34Wg
Soru 1
Titanik’deki yolcu sayısını bulunuz.
Soru 2
Titanik’deki yolcuların kaç tanesi kurtulmuştur ?
Soru 3
Kurtulan yolcuların kaç tanesi erkek, kaç tanesi kadındır ?
Soru 4
Hayatını kaybeden yolcu sayısı kaçtır ?
Soru 5
Hayatını kaybeden çocuk yolcuların isimlerini bulunuz..

0-7 Yaş aralığını çocuk kabul ediniz.

--Apache Hive--
Büyük veriler ile hadoop üzerinde işlem yapmak için kullanılır.
SQl sorguları yazılı.
1- VErileri hdfs'e at 2- veritabanı oluştur  3- Tablo Oluştur 4- Sql sorgusu oluştur.

hdfs dfs -copyFromLocal /home/cloudera/Desktop/movies.csv /hiveexam

veritabanı oluşturmak için browserda hue linkine tıkla. + ile veritabanı oluştur. Dbye gir table oluştur ve hdfs deki csvyi sec.
query ile sql sorguları artık yazılabilir.

Select * from movie.movies
Select title from movie.movies
Select title,genres from movie.movies where genres='Comedy'
Select title,genres from movie.movies where genres LIKE '%Comedy%'
Select count(title) from movie.movies where genres LIKE '%Comedy%'
Select title,genres from movie.movies where title LIKE '%1996%'
Select title,genres from movie.movies where title LIKE '%1996%' AND genres LIKE '%Comedy%'
Select count(title) from movie.movies where title LIKE '%1996%' AND genres LIKE '%Comedy%'

ödev
https://yadi.sk/d/NBeURuPa3V3Rp3
Soru 1
Toplam kaç yorum yapılmıştır ?
Soru 2
Hangi şehirden kaç yorum yapılmıştır ?
Soru 3
Kullanıcı yorumlarından olumlu olan yorumları seçiniz

Spark,Hadooptan daha hızlıdır.Çünkü veri saklama birimi olmadığı için Memory'de (Ram üzerinde) çalışır.

--MongoDb--
document-oriented,her kayıt bir dökümandır,dökümanlar json olarak saklanır.ilişti kurmaya gerek yok.
cluster yapılabilir, replication yapabiliyoruz. javada primefaces ile, nodejs ile entegre edilebilir.

MongoDB Windows Kurulumu
MongoDB'yi bilgisayarımıza kurmak için, ilk olarak MongoDB Download sayfasına gidelim.
Buradan işletim sistemimize uygun olan dosyayı bilgisayarımıza indirelim.
İndirdiğimiz dosyayı çalıştıralım ve Next>Lisansı kabul edelim>Next diyerek ilerleyelim.
Bu aşamaya Complete seçeneğini seçelim.Sol alt tarafta bulunan Install MongoDB Compass seçeneğindeki tiki kaldıralım ve Next diyelim.
C:'nin altında mongodb adında klasör oluşturalım ve içerisine mongo.config ve mongo.log adında iki dosya oluşturalım.
C:'nin altında ki mongodb klasörünün içerisine data adında klasör oluşturalım
mongo.config dosyasını açalım ve içerisine ;
dbpath = C:\mongodb\data
logpath = C:\mongodb\mongo.log         satırlarını ekleyelim.
 Kurulum işlemi tamamlandı.

MongoDB sunucusunu çalıştırmak için ;
Windows komut işlemini açalım ( cmd ) 
Komut satırında mongo kurulum path'ine geldikten sonra ( cd C:\mongodb )
mongod.exe --config C:\mongodb\mongo.config        satırını yazarak enter'a basalım.

arayüz olarak Robo 3T kullanılmalı...

https://github.com/talhaklc/Mongodb-Java-Enteg
java Maven-- kütüphane bağımlılıklarını ortadan kaldırır. pom.xml e isteiğin kütüphaneyi yaz o netten indirir.
googledan aşağıdaki dependecy bulduk.
https://mvnrepository.com/artifact/org.mongodb/mongo-java-driver
pom.xml e dependecy tagını ekledik. Spring Tool Suit menulerden Project + Build yapılınca yeni eklenen dependecyler indirilir.
https://github.com/talhaklc/Mongodb-Java-Enteg

-- Elasticsearch --
database = index deniyor. Tablo = Type diye hitap ediliyor.
Elasticsearch herhangi bir konfigürasyon yapmadan kurulduğunda default olarak 5 shard dan oluşur.
Elasticsearch'de index oluşturmak için GET /IndexName
Elastic de nosql bir db dir. Çok hızlı okuma yapar. Çünkü veriler yazılırken indexleme yapılır. Ama yazma hızı yavaştır.
https://www.elastic.co/
verileri json şeklinde tutar.
Kurulum için
https://umutcakir.wordpress.com/2017/02/25/ubuntu-16-04-server-uzerinde-elasticsearch-kurulumu-ve-ram-kullanimi-ayarlariheap-size/
https://www.elastic.co/downloads/elasticsearch
https://hub.docker.com/_/elasticsearch
Sunucu 9200 portundan çalışıyor. POST,GET,PUT,DELETE işlemleri yapılabilir. API yazmış gibi olduk.:)
Postmen ile test yapılabilir. örnek GET localhost:9200/_all  //tüm veritabanı bilgilerini verir.
GET localhost:9200/exam/product/_search?q=brand:apple  //tüm apple markalı ürünleri listeler.
https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html
docker container run -d elasticsearch -p 9200:9200 --name elastic 
java Maven ile yazarken elasticsearch verionu kaçsa java versionunu ona göre seçmek lazım
dependecy olarak Mavenrepository.com dan Transport seçilmeli.
https://github.com/talhaklc/elasticsearch-java-ent
http://www.borakasmer.com/net-core-uzerinde-elasticsearch-ile-istenen-bir-mesafe-icindeki-lokasyonlarin-filitrelenmesi/
